{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Error Handling for Sequence of Different Length:"
      ],
      "metadata": {
        "id": "8jQYZlDPxsM1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "gdhDAaAItRkk",
        "outputId": "d2b15053-e245-46a3-d1f5-998b80d6d19c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "expected sequence of length 11 at dim 1 (got 12)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-80c0b872a6ef>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'token_ids:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ids:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: expected sequence of length 11 at dim 1 (got 12)"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "input = [\n",
        "    'Virat Kohli is the best cricketer in the world',\n",
        "    'Rohit sharma has the highest individual score in ODI format cricket',\n",
        "    ]\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "tokens=[tokenizer.tokenize(seq) for seq in input]\n",
        "ids = [tokenizer.convert_tokens_to_ids(token)  for token in tokens]\n",
        "input_ids = torch.tensor(ids)\n",
        "print('token_ids:',tokens)\n",
        "print('ids:',ids)\n",
        "print('input_ids:',input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling the above error:\n",
        "## By Padding:"
      ],
      "metadata": {
        "id": "Y0iKjLZex3JG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('id_1:',ids[0])\n",
        "print('id_2',ids[1])\n",
        "print('ids:',ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqQVBHqrtkJ-",
        "outputId": "a5f79e0c-0fa4-4bef-fb28-8a378b714c53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id_1: [6819, 8609, 12849, 27766, 2003, 1996, 2190, 9490, 1999, 1996, 2088]\n",
            "id_2 [20996, 16584, 14654, 2038, 1996, 3284, 3265, 3556, 1999, 21045, 4289, 4533]\n",
            "ids: [[6819, 8609, 12849, 27766, 2003, 1996, 2190, 9490, 1999, 1996, 2088], [20996, 16584, 14654, 2038, 1996, 3284, 3265, 3556, 1999, 21045, 4289, 4533]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id_1 = torch.tensor([[6819, 8609, 12849, 27766, 2003, 1996, 2190, 9490, 1999, 1996, 2088]])\n",
        "\n",
        "id_2 = torch.tensor([[20996, 16584, 14654, 2038, 1996, 3284, 3265, 3556, 1999, 21045, 4289, 4533]])\n",
        "\n",
        "ids = torch.tensor([[6819, 8609, 12849, 27766, 2003, 1996, 2190, 9490, 1999, 1996, 2088,tokenizer.pad_token_id],\n",
        "                    [20996, 16584, 14654, 2038, 1996, 3284, 3265, 3556, 1999, 21045, 4289, 4533]])\n",
        "\n",
        "print('id_1:',id_1)\n",
        "print('id_2',id_2)\n",
        "print('batch_ids:',ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwlttGx8zLH3",
        "outputId": "40f30967-9ed4-4dc0-f3d3-3ab93326a5ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id_1: tensor([[ 6819,  8609, 12849, 27766,  2003,  1996,  2190,  9490,  1999,  1996,\n",
            "          2088]])\n",
            "id_2 tensor([[20996, 16584, 14654,  2038,  1996,  3284,  3265,  3556,  1999, 21045,\n",
            "          4289,  4533]])\n",
            "batch_ids: tensor([[ 6819,  8609, 12849, 27766,  2003,  1996,  2190,  9490,  1999,  1996,\n",
            "          2088,     0],\n",
            "        [20996, 16584, 14654,  2038,  1996,  3284,  3265,  3556,  1999, 21045,\n",
            "          4289,  4533]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model(id_1).logits)\n",
        "print(model(id_2).logits)\n",
        "print(model(ids).logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTp1M-5R1DzJ",
        "outputId": "a0445dd5-b7a1-47c5-f851-2cffa6dd4a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-3.9132,  4.3216]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[-2.4203,  3.0150]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[-3.8709,  4.2965],\n",
            "        [-2.4203,  3.0150]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see from above that the logit of id where we have added the padding is not same as logit of individual id.\n",
        "\n",
        "There’s something wrong with the logits in our batched predictions: the first row should be the same as the logits for the second sentence, but we’ve got completely different values!\n",
        "\n",
        "This is because the key feature of Transformer models is attention layers that contextualize each token. These will take into account the padding tokens since they attend to all of the tokens of a sequence. To get the same result when passing individual sentences of different lengths through the model or when passing a batch with the same sentences and padding applied, we need to tell those attention layers to ignore the padding tokens. This is done by using an attention mask."
      ],
      "metadata": {
        "id": "JNQglg3c4I4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## By Attention Masking:"
      ],
      "metadata": {
        "id": "Qq1B4tF84ipe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention_mask = torch.tensor([\n",
        "    [1,1,1,1,1,1,1,1,1,1,1,0],\n",
        "    [1,1,1,1,1,1,1,1,1,1,1,1]])\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "print('Logits:',model(ids,attention_mask=attention_mask).logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O50Y2lrZ1KOA",
        "outputId": "77573bca-bb1d-4992-9755-dba68c136f2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits: tensor([[-3.9132,  4.3216],\n",
            "        [-2.4203,  3.0150]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we get the same logits for the first sentence in the batch.\n",
        "\n",
        "Attention masks are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to (i.e., they should be ignored by the attention layers of the model)."
      ],
      "metadata": {
        "id": "lXmHJKU36-wg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Longer Sequences:\n",
        "With Transformer models, there is a limit to the lengths of the sequences we can pass the models. Most models handle sequences of up to 512 or 1024 tokens, and will crash when asked to process longer sequences. There are two solutions to this problem:\n",
        "\n",
        "Use a model with a longer supported sequence length.\n",
        "Truncate your sequences."
      ],
      "metadata": {
        "id": "TgRZjig_7TV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting All Together:"
      ],
      "metadata": {
        "id": "yerj8OTf8hXQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Trying Padding and Truncation:"
      ],
      "metadata": {
        "id": "5YaD74RR_ibk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "input= ['Hi my name is Pavan sai',\n",
        "        'i love virat kohli and he is my inspiration']\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "oKAysqqS8o0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Here we are trying padding with logest length of sentence\n",
        "input_ids = tokenizer(input, padding='longest',return_tensors='pt')\n",
        "output_ids = model(**input_ids)\n",
        "\n",
        "print(output_ids.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMkuFmBq91Ac",
        "outputId": "27916ceb-c1cc-4818-e9e0-69022eb12ee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-2.4683,  2.7601],\n",
            "        [-4.1838,  4.5280]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Here we are trying padding with the max length of model\n",
        "input_ids= tokenizer(input, padding='max_length', return_tensors='pt')\n",
        "output_ids = model(**input_ids)\n",
        "print(output_ids.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAYUM94_-Z26",
        "outputId": "21d8f593-fc90-4ef4-e23d-a6dc54b31496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-2.4683,  2.7601],\n",
            "        [-4.1838,  4.5280]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Here we are trying padding with the max length but we have also specified max length and truncation\n",
        "# Will truncate the sequences that are longer than the model max length\n",
        "input_ids= tokenizer(input, padding='max_length', max_length =4, truncation =True, return_tensors='pt')\n",
        "output_ids = model(**input_ids)\n",
        "print(output_ids.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsKoMu7n-vT0",
        "outputId": "2ff2369a-079d-481d-ea7e-6bbc4f5170c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-3.2258,  3.4330],\n",
            "        [-4.3595,  4.7084]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Trying various tensor formats:"
      ],
      "metadata": {
        "id": "LJen38Na_pru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Here tf indicates tensor flow tensors\n",
        "input_ids= tokenizer(input, padding= True, return_tensors='tf')\n",
        "print(input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oDGLsgJ-1EC",
        "outputId": "9d76d498-dfd6-4cf3-ca6e-dd6a850c717c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': <tf.Tensor: shape=(2, 13), dtype=int32, numpy=\n",
            "array([[  101,  7632,  2026,  2171,  2003,  6643,  6212, 18952,   102,\n",
            "            0,     0,     0,     0],\n",
            "       [  101,  1045,  2293,  6819,  8609, 12849, 27766,  1998,  2002,\n",
            "         2003,  2026,  7780,   102]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 13), dtype=int32, numpy=\n",
            "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Here tf indicates pytorch tensors\n",
        "input_ids= tokenizer(input, padding= True, return_tensors='pt')\n",
        "print(input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvRyoWnj_3V4",
        "outputId": "4ea85409-c7d2-492c-9292-6036cb097333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  7632,  2026,  2171,  2003,  6643,  6212, 18952,   102,     0,\n",
            "             0,     0,     0],\n",
            "        [  101,  1045,  2293,  6819,  8609, 12849, 27766,  1998,  2002,  2003,\n",
            "          2026,  7780,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Here tf indicates numpy tensors\n",
        "input_ids= tokenizer(input, padding= True, return_tensors='np')\n",
        "print(input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdhSx1APAKfJ",
        "outputId": "82b3a00a-cf4c-4a60-ab9a-573c8562f1d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': array([[  101,  7632,  2026,  2171,  2003,  6643,  6212, 18952,   102,\n",
            "            0,     0,     0,     0],\n",
            "       [  101,  1045,  2293,  6819,  8609, 12849, 27766,  1998,  2002,\n",
            "         2003,  2026,  7780,   102]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Special Tokens:"
      ],
      "metadata": {
        "id": "goZTM-WjAQpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing multiple sentences\n",
        "model_inputs = tokenizer(input, padding = True, truncation=True, return_tensors = 'pt')\n",
        "# Decode the batch of tokenized sentences\n",
        "print('model_inputs:\\n', tokenizer.batch_decode(model_inputs['input_ids']))\n",
        "\n",
        "# Tokenizing manually and converting to IDs\n",
        "tokens = [tokenizer.tokenize(token) for token in input]\n",
        "print('\\n token_id:',tokens)\n",
        "\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(id) for id in tokens]\n",
        "print('\\ninput_ids:',input_ids)\n",
        "\n",
        "print('\\ndecoded text:',[tokenizer.decode(id) for id in input_ids])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2VJzDn0AN7d",
        "outputId": "25ede75d-516d-42a0-ad5a-712255fc2859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_inputs:\n",
            " ['[CLS] hi my name is pavan sai [SEP] [PAD] [PAD] [PAD] [PAD]', '[CLS] i love virat kohli and he is my inspiration [SEP]']\n",
            "\n",
            " token_id: [['hi', 'my', 'name', 'is', 'pa', '##van', 'sai'], ['i', 'love', 'vi', '##rat', 'ko', '##hli', 'and', 'he', 'is', 'my', 'inspiration']]\n",
            "\n",
            "input_ids: [[7632, 2026, 2171, 2003, 6643, 6212, 18952], [1045, 2293, 6819, 8609, 12849, 27766, 1998, 2002, 2003, 2026, 7780]]\n",
            "\n",
            "decoded text: ['hi my name is pavan sai', 'i love virat kohli and he is my inspiration']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BPm932YLDdoP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}